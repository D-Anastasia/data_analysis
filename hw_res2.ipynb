{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests import request\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import functools\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from itertools import chain\n",
    "import re\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = list(map(str.strip, open('authors.txt', 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(func):\n",
    "    @functools.wraps(func)\n",
    "    def newfunc(*args,**kwargs):\n",
    "        res = []\n",
    "        try:\n",
    "            res = func(*args,**kwargs)\n",
    "        except:\n",
    "            print(\"Error\", file=sys.stderr)\n",
    "        return res\n",
    "    return newfunc\n",
    "\n",
    "@safe\n",
    "def get_page(url, n_attempts=5, t_sleep=1, **kwargs):\n",
    "    r_get = requests.get(url, params=kwargs)\n",
    "    if not r_get.ok:\n",
    "        for attempt in range(n_attempts):\n",
    "            time.sleep(t_sleep)\n",
    "            r_get = requests.get(url, params=kwargs)\n",
    "            if r_get:\n",
    "                break\n",
    "    return r_get\n",
    "@safe\n",
    "def get_cur_list(url):\n",
    "    page = get_page(url, n_attempts=5, t_sleep=1)\n",
    "    if not page.ok:\n",
    "        print(url, file=sys.stderr)\n",
    "        return []\n",
    "    author_page = page.text\n",
    "    soup = BeautifulSoup(author_page, 'html.parser')\n",
    "    soup = BeautifulSoup(author_page, 'lxml')\n",
    "    return [e.attrs['href'] for e in soup.find_all('a', class_=\"rd-listing-product-item__image-wrapper\")]\n",
    "\n",
    "@safe\n",
    "def get_list(url):\n",
    "    links = []\n",
    "    url = 'https://www.respublica.ru/authors/' + url\n",
    "    page = get_page(url, n_attempts=5, t_sleep=1)\n",
    "    if not page.ok:\n",
    "        print(url, file=sys.stderr)\n",
    "        return []\n",
    "    author_page = page.text\n",
    "    soup = BeautifulSoup(author_page, 'html.parser')\n",
    "    number = soup.find('span',class_ = \"rd-listing-count__total\").text\n",
    "    cur_page = 1\n",
    "    num = int(number)\n",
    "\n",
    "    while len(links)<num:\n",
    "        cur_url = str(url) +'?page=' +str(cur_page)\n",
    "        links+=get_cur_list(cur_url)\n",
    "        cur_page+=1\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b097a714431436ba85c986f0ae7c14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "with ThreadPool(10) as pool:\n",
    "    max_ = len(authors)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for obr, cur_list in enumerate(pool.imap_unordered(get_list, authors)):\n",
    "            pages.append(cur_list)\n",
    "            pbar.update()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@safe\n",
    "def process_page(url):\n",
    "    url = 'https://www.respublica.ru' + url\n",
    "    \n",
    "    page = get_page(url, n_attempts=5, t_sleep=1)\n",
    "    if not page.ok:\n",
    "        print(url, file=sys.stderr)\n",
    "        return {}\n",
    "    book_page = page.text\n",
    "    soup = BeautifulSoup(book_page, 'lxml')\n",
    "    \n",
    "    book_info = {}\n",
    "    book_info[\"ID\"] = soup.find('div',class_=\"rd-page-product__article\").find('span', itemprop=\"sku\").text\n",
    "    book_info[\"URL\"] = url\n",
    "    book_info[\"Название\"] = soup.find('h1', class_=\"rd-page-product__title\").text\n",
    "    book_info[\"Автор\"] = ';'.join([e.text for e in soup.find_all('a',itemprop=\"brand\")])\n",
    "    \n",
    "    preview = soup.find('div', class_=\"rd-page-product__pages-preview-container hidden\")\n",
    "    if preview:\n",
    "        preview = preview.find('a',class_=\"download-pdf\").attrs['href']\n",
    "        book_info[\"Превью\"] = 'https://www.respublica.ru' + preview\n",
    "    \n",
    "    image = soup.find('img', class_=\"rd-page-product__img\").attrs['data-zoom-image']\n",
    "    book_info[\"Изображение\"] = 'https://www.respublica.ru' + image\n",
    "    \n",
    "    book_info[\"Описание\"] = soup.find('div', class_=\"rd-page-product__desc-body\" ).text\n",
    "    book_info[\"Цена\"] = int(soup.find('meta',itemprop=\"price\").attrs['content'])\n",
    "    \n",
    "    price_old = soup.find('div', class_=\"rd-page-product__price-old\")\n",
    "    if price_old:\n",
    "        price_old = price_old.find('span',class_=\"prev\").text\n",
    "        book_info[\"Цена (старая)\"] = int(re.split(r' ', price_old)[0])\n",
    "        \n",
    "    available = soup.find('div', class_=\"rd-page-product__buttons\").find('a').attrs['class']\n",
    "    book_info[\"В наличии\"] = 'rd-page-product__buy_status_available' in available\n",
    "    \n",
    "    categories = soup.find_all('span', class_=\"rd-page-breadcrumbs-item\")\n",
    "    book_info[\"Категория\"] = ';'.join(e.find('span', itemprop=\"name\").text for e in categories)\n",
    "    \n",
    "    rating = soup.find('span',itemprop=\"aggregateRating\")\n",
    "    if rating:\n",
    "        book_info[\"Число отзывов\"] = float(rating.find('meta',itemprop='reviewCount').attrs['content'])\n",
    "        book_info[\"Число оценок\"] = float(rating.find('meta',itemprop='ratingCount').attrs['content'])\n",
    "        book_info[\"Оценка\"] = float(rating.find('meta',itemprop='ratingValue').attrs['content'])\n",
    "        \n",
    "    table = soup.find('div', class_=\"rd-page-product__desc-params\")\n",
    "    for row in table.find_all('p', class_=\"rd-page-product__desc-param\"):\n",
    "        key = row.find(itemprop='name').text\n",
    "        val = row.find(itemprop='value').text\n",
    "        book_info[key] = val\n",
    "    return book_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2456"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = list(chain.from_iterable(pages))\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888f8521032844e09f7a0507c1eeedb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2456.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "with ThreadPool(10) as pool:\n",
    "    max_ = len(urls)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for obr, book_inf in enumerate(pool.imap_unordered(process_page, urls)):\n",
    "            result.append(book_inf)\n",
    "            pbar.update()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 ms, sys: 12.4 ms, total: 127 ms\n",
      "Wall time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.DataFrame(result)\n",
    "df.sort_values(by=['ID'], inplace=True)\n",
    "\n",
    "with open('data/hw_3.csv', mode='w', encoding='utf-8') as f_csv:\n",
    "    df.to_csv(f_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "result[1] = ' '.join(['ixgnu','xbnie','wndi'])\n",
    "result[2] = ' '.join(['ixgnu2','xbni2e','wn2di','nxferbi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/hw_311111111111.csv', mode='w', encoding='utf-8') as f_csv:\n",
    "    df.to_csv(f_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.csv','w') as out:\n",
    "    for key,val in result.items():\n",
    "        out.write('{},{}\\n'.format(key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
